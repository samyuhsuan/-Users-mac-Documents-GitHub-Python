{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndata = pd.read_csv(\"NextData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndata = ndata.drop(['Advertiser', 'Advertiser ID', 'Advertiser Status',\n",
    "       'Advertiser Integration Code',\n",
    "       'Advertiser Currency'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndata.columns = ['TOD', 'Country', 'Device', 'Impressions', 'mImpressions', 'vImpressions', 'Clicks','Spend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndata[\"CTR\"] = ndata.Clicks / ndata.Impressions\n",
    "ndata[\"Viewability\"] = ndata.vImpressions / ndata.mImpressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndata = ndata[ndata.Device.isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndata.Country = ndata.Country.astype(\"category\")\n",
    "ndata.Device = ndata.Device.astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#craete dependent variable matrix X and independent variable y\n",
    "#here importantly we need to transform the dataframe into array of values before it can be used\n",
    "X = ndata.loc[:, [\"TOD\",\"Viewability\", \"Device\",]].values\n",
    "y = ndata.loc[:, \"CTR\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replacing missing data with mean\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#firstly we define the imputer object with Imputer and the criteria\n",
    "#firstly fit the imputer to X dataset which will save imputer object with X mean etc. value in it\n",
    "imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis = 0)\n",
    "imputer = imputer.fit(X[:, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#then we set X equals to X being transformed by using the data saved imputer object to X\n",
    "X[:, 0:2] = imputer.transform(X[:, 0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we fix categorical data and transform that to dummy arrays\n",
    "#import LabelEncoder to transform categorical data to numerical\n",
    "#import OneHotEncoder to then take the numerical and make that to dummy with columns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#firstly use LabelEncoder to fit and transform non-numerical data to numerical\n",
    "labelencoder_x = LabelEncoder()\n",
    "X[:, 2] = labelencoder_x.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#then use OneHotEncoder to fit and transform the numerical data to dummy \n",
    "#OneHotEncoder syntax the parameter categorical_features=[g], g is the column we like to create dummy against\n",
    "dummy = OneHotEncoder(categorical_features=[2])\n",
    "X = dummy.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#below syntax with above cross_validation is to use to split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import StandardScaler this is to standardise the dataset \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#firstly fir the standard scaler to the training dataset\n",
    "X_train = sc.fit_transform(X_train)\n",
    "#as sc is now fit with X training set, there is no need to fit it again, just use it to transform test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regressor_ = regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = regressor_.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------Backward elimination-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here we go through the process of backward elimination to build an optimal model which takes in only the optimsal independent variables\n",
    "#import the statsmodel here to use its OLS to check \n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#one thing to bear in mind is that OLS doesn't take into account constant \n",
    "#we need to therefore build a column of 1s for it factor in constant\n",
    "#trick: here we reverse the arguments in append\n",
    "#by append X to the value instead of value to X we get the 1s to the first column\n",
    "#np.ones() output is an array, so use astype(int) to convert that into a list of values so X can append to it\n",
    "X = np.append(arr=np.ones((len(X[:,0]), 1)).astype(int), values = X, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now we can through the process of fitting OLS to our independent variables\n",
    "#firstly we create an object to represent the dataset X instead of using X directly\n",
    "#this is important in the sense that so when we take out the each independent variables we can keep track of them\n",
    "X_opt = X[:, [1, 2, 3, 4, 5, 6]]\n",
    "#then we fit the OLS model to the dataset\n",
    "regressor_summary = sm.OLS(endog = y, exog = X_opt).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.132</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Apr 2017</td> <th>  Prob (F-statistic):</th> <td>1.56e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:03:00</td>     <th>  Log-Likelihood:    </th> <td>  1926.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   336</td>      <th>  AIC:               </th> <td>  -3842.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   331</td>      <th>  BIC:               </th> <td>  -3823.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.0008</td> <td>    0.000</td> <td>    3.622</td> <td> 0.000</td> <td>    0.000     0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 7.479e-07</td> <td> 9.58e-05</td> <td>    0.008</td> <td> 0.994</td> <td>   -0.000     0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0011</td> <td>    0.000</td> <td>    8.355</td> <td> 0.000</td> <td>    0.001     0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0003</td> <td>    0.000</td> <td>   -1.932</td> <td> 0.054</td> <td>   -0.001  5.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>-1.623e-07</td> <td>  6.3e-06</td> <td>   -0.026</td> <td> 0.979</td> <td>-1.25e-05  1.22e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0007</td> <td>    0.001</td> <td>    1.150</td> <td> 0.251</td> <td>   -0.001     0.002</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>336.547</td> <th>  Durbin-Watson:     </th> <td>   2.140</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>13081.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 4.190</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>32.397</td>  <th>  Cond. No.          </th> <td>5.55e+16</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.132\n",
       "Model:                            OLS   Adj. R-squared:                  0.121\n",
       "Method:                 Least Squares   F-statistic:                     12.57\n",
       "Date:                Sun, 16 Apr 2017   Prob (F-statistic):           1.56e-09\n",
       "Time:                        17:03:00   Log-Likelihood:                 1926.1\n",
       "No. Observations:                 336   AIC:                            -3842.\n",
       "Df Residuals:                     331   BIC:                            -3823.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0008      0.000      3.622      0.000         0.000     0.001\n",
       "x1          7.479e-07   9.58e-05      0.008      0.994        -0.000     0.000\n",
       "x2             0.0011      0.000      8.355      0.000         0.001     0.001\n",
       "x3            -0.0003      0.000     -1.932      0.054        -0.001  5.83e-06\n",
       "x4         -1.623e-07    6.3e-06     -0.026      0.979     -1.25e-05  1.22e-05\n",
       "x5             0.0007      0.001      1.150      0.251        -0.001     0.002\n",
       "==============================================================================\n",
       "Omnibus:                      336.547   Durbin-Watson:                   2.140\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            13081.910\n",
       "Skew:                           4.190   Prob(JB):                         0.00\n",
       "Kurtosis:                      32.397   Cond. No.                     5.55e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.98e-29. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#then we examine the summary of it\n",
    "regressor_summary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   46.06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 16 Apr 2017</td> <th>  Prob (F-statistic):</th> <td>5.25e-11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:04:15</td>     <th>  Log-Likelihood:    </th> <td>  1924.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   336</td>      <th>  AIC:               </th> <td>  -3844.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   334</td>      <th>  BIC:               </th> <td>  -3836.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.0011</td> <td> 4.48e-05</td> <td>   25.348</td> <td> 0.000</td> <td>    0.001     0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.0011</td> <td>    0.000</td> <td>    6.786</td> <td> 0.000</td> <td>    0.001     0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>331.807</td> <th>  Durbin-Watson:     </th> <td>   2.160</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>12295.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 4.114</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>31.470</td>  <th>  Cond. No.          </th> <td>    3.90</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.121\n",
       "Model:                            OLS   Adj. R-squared:                  0.119\n",
       "Method:                 Least Squares   F-statistic:                     46.06\n",
       "Date:                Sun, 16 Apr 2017   Prob (F-statistic):           5.25e-11\n",
       "Time:                        17:04:15   Log-Likelihood:                 1924.1\n",
       "No. Observations:                 336   AIC:                            -3844.\n",
       "Df Residuals:                     334   BIC:                            -3836.\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.0011   4.48e-05     25.348      0.000         0.001     0.001\n",
       "x1             0.0011      0.000      6.786      0.000         0.001     0.001\n",
       "==============================================================================\n",
       "Omnibus:                      331.807   Durbin-Watson:                   2.160\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            12295.611\n",
       "Skew:                           4.114   Prob(JB):                         0.00\n",
       "Kurtosis:                      31.470   Cond. No.                         3.90\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt = X[:, [1, 3]]\n",
    "regressor_summary = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_summary.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.26607072],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.39137441],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         1.        ,  0.25305692],\n",
       "       ..., \n",
       "       [ 1.        ,  1.        ,  0.        , ...,  0.        ,\n",
       "         9.        ,  0.41634366],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         9.        ,  0.46174584],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.        ,\n",
       "         9.        ,  0.4109987 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
